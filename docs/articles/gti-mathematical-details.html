<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>GTI mathematical details • MALECOT</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="GTI mathematical details">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">MALECOT</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Vignettes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/installation.html">Installation</a>
    </li>
    <li>
      <a href="../articles/tutorial-biallelic.html">Tutorial 1: bi-allelic data</a>
    </li>
    <li>
      <a href="../articles/complex-priors.html">Tutorial 2: more realistic models</a>
    </li>
    <li>
      <a href="../articles/multiallelic-data.html">Tutorial 3: multi-allelic data</a>
    </li>
    <li>
      <a href="../articles/comparing-models.html">Tutorial 4: comparing models (estimating K)</a>
    </li>
    <li>
      <a href="../articles/running-in-parallel.html">Tutorial 5: running in parallel</a>
    </li>
    <li class="divider">
    <li class="dropdown-header">Advanced topics</li>
    <li>
      <a href="../articles/power-analysis-within-model.html">Power analysis - within model</a>
    </li>
    <li>
      <a href="../articles/power-analysis-between-models.html">Power analysis - between models</a>
    </li>
    <li>
      <a href="../articles/gti-mathematical-details.html">GTI mathematical details</a>
    </li>
  </ul>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li>
  <a href="../articles/faq.html">FAQ</a>
</li>
<li>
  <a href="../articles/known-issues.html">Known issues</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/bobverity/malecot">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>GTI mathematical details</h1>
                        <h4 class="author">Bob Verity</h4>
            
            <h4 class="date">2018-09-25</h4>
      
      
      <div class="hidden name"><code>gti-mathematical-details.Rmd</code></div>

    </div>

    
    
<p>This vignette gives complete mathematical details of the model comparison methods implemented in MALECOT. It covers:</p>
<ol style="list-style-type: decimal">
<li>problem definition - model evidence and Bayes factors</li>
<li>thermodynamic integration (TI)</li>
<li>generalised thermodynamic integration (GTI)</li>
</ol>
<p>Most of this derivation is taken from <a href="http://www.genetics.org/content/203/4/1827.supplemental">Supplementary Material 1</a> of (ref), which also contains derivations of common model comparison statistics.</p>
<div id="problem-definition" class="section level2">
<h2 class="hasAnchor">
<a href="#problem-definition" class="anchor"></a>Problem definition</h2>
<p>We start by assuming a set of models <span class="math inline">\(\mathcal{M}_k\)</span> for <span class="math inline">\(k \in 1:K\)</span>, each of which has its own set of parameters <span class="math inline">\(\theta_k\)</span>. The probability of the observed data <span class="math inline">\(x\)</span> conditional on the model and the parameters of the model (a.k.a. the likelihood) can be written <span class="math inline">\(\mbox{Pr}(x \: | \: \theta_k, \mathcal{M}_k)\)</span>. We are interested in comparing models based on the <em>Bayesian evidence</em>, defined as the probability of the model integrated over all free parameters (a.k.a. the marginal likelihood of the model). This can be written</p>
<p><span class="math display">\[
\begin{align}
    \hspace{30mm} \mbox{Pr}(x \:|\: \mathcal{M}_k) = \int_{\scriptsize\theta_k} \mbox{Pr}(x \:|\: \theta_k, \mathcal{M}_k) \mbox{Pr}(\theta_k \:|\: \mathcal{M}_k) \:d\theta_k \;. \hspace{30mm}[1]
\end{align}
\]</span> Once we have this quantity it is straightforward to compute bayes factors as the ratio of evidence between models:</p>
<p><span class="math display">\[
\hspace{30mm} \mbox{BF}(\mathcal{M}_i:\mathcal{M}_j) = \frac{\mbox{Pr}(x \:|\: \mathcal{M}_i)}{\mbox{Pr}(x \:|\: \mathcal{M}_j)} \;, \hspace{30mm}[2]
\]</span> or to combine the evidence with a prior over models to arrive at the posterior probability of a model:</p>
<p><span class="math display">\[
\hspace{30mm} \mbox{Pr}(\mathcal{M}_k \:|\: x) = \frac{\mbox{Pr}(x \:|\: \mathcal{M}_k)}{ \sum_{i=1}^K \mbox{Pr}(x \:|\: \mathcal{M}_i) } \;. \hspace{30mm}[3]
\]</span> The challenging part in this analysis plan is the integral in (X), which is often extremely high-dimensional making it computationally infeasible by standard methods. Fortunately we can use advanced MCMC methods to arrive at asymptotically unbiased estimates of <span class="math inline">\(\log[\mbox{Pr}(x \:|\: \mathcal{M}_k)]\)</span>, from which we can derive the other quantities of interest.</p>
<p>In the following sections, <span class="math inline">\(\mathcal{M}_k\)</span> and <span class="math inline">\(\theta_k\)</span> are often shortened to <span class="math inline">\(\mathcal{M}\)</span> and <span class="math inline">\(\theta\)</span> for succinctness, in which case it is assumed that the model <span class="math inline">\(\mathcal{M}\)</span> is one of several to be compared.</p>
</div>
<div id="thermodynamic-integration" class="section level2">
<h2 class="hasAnchor">
<a href="#thermodynamic-integration" class="anchor"></a>Thermodynamic integration</h2>
<p>Thermodynamic Integration (TI) provides direct estimates of the (log) model evidence that are unbiased and have finite and quantifiable variance. The method exploits the “power posterior” , defined as follows:</p>
<p><span class="math display">\[
\begin{eqnarray}
    \hspace{30mm} P_\beta(\theta \:|\: x, \mathcal{M}) &amp;= \dfrac{\mbox{Pr}( x \:|\: \theta, \mathcal{M})^\beta \: \mbox{Pr}( \theta \:|\: \mathcal{M})}{u( x \:|\: \beta, \mathcal{M})} \;. \hspace{30mm}[4]
\end{eqnarray}
\]</span></p>
<p>where <span class="math inline">\(u(x \:|\: \beta, \mathcal{M})\)</span> is a normalising constant that ensures the distribution integrates to unity:</p>
<p><span class="math display">\[
\begin{eqnarray*}
    \hspace{30mm} u( x \:|\: \beta, \mathcal{M}) &amp;= \displaystyle\int_{\scriptsize\theta} \mbox{Pr}( x \:|\: \theta, \mathcal{M})^\beta \: \mbox{Pr}( \theta \:|\: \mathcal{M}) \:d\theta \;. \hspace{30mm}[5]
\end{eqnarray*}
\]</span></p>
<p>In subsequent expressions in this section, conditioning on the model <span class="math inline">\(\mathcal{M}\)</span> will be supressed for succinctness.</p>
<p>The crucial step in the TI method is the following derivation:</p>
<p><span class="math display">\[
\begin{eqnarray}
    \frac{d}{d\beta} \log[u( x \:|\: \beta )] &amp;=&amp; \frac{1}{u( x \:|\: \beta )} \: \frac{d}{d\beta} u( x \:|\: \beta ) \;, \hspace{56mm} \color{gray}{\textit{(by chain rule)}} \\[4mm]
    &amp;=&amp; \frac{1}{u( x \:|\: \beta )} \: \frac{d}{d\beta} \int_{\scriptsize\theta} \: \mbox{Pr}( x \:|\: \theta )^\beta \: \mbox{Pr}( \theta ) \:d\theta \;, \hspace{30mm} \color{gray}{\textit{(substituting in (X))}} \\[4mm]
    &amp;=&amp; \frac{1}{u( x \:|\: \beta )} \: \int_{\scriptsize\theta} \: \frac{d}{d\beta} \mbox{Pr}( x \:|\: \theta )^\beta \: \mbox{Pr}( \theta ) \:d\theta \;, \hspace{30mm} \color{gray}{\textit{(by Leibniz integral rule)}} \\[4mm]
    &amp;=&amp; \dfrac{1}{u( x \:|\: \beta )} \: \displaystyle\int_{\scriptsize\theta}  \mbox{Pr}( x \:|\: \theta )^\beta \log[ \mbox{Pr}( x \:|\: \theta ) ] \: \mbox{Pr}( \theta ) \:d\theta \;, \hspace{14mm} \color{gray}{\textit{(evaluating derivative)}} \\[4mm]
    &amp;=&amp; \int_{\scriptsize\theta} \log[ \mbox{Pr}( x \:|\: \theta ) ] \frac{\mbox{Pr}( x \:|\: \theta )^\beta \mbox{Pr}( \theta ) }{u( x \:|\: \beta )} \:d\theta \;, \hspace{30mm} \color{gray}{\textit{(rearranging)}} \\[4mm]
    &amp;=&amp; \int_{\scriptsize\theta} \log[ \mbox{Pr}( x \:|\: \theta ) ] P_\beta(\theta \:|\: x) \:d\theta \;, \hspace{42mm} \color{gray}{\textit{(substituting in (X))}} \\[4mm]
    &amp;=&amp; \mbox{E}_{\scriptsize\theta \,|\, x, \beta}\Big[ \log[ \mbox{Pr}( x \:|\: \theta ) ] \Big] \;. \hspace{54mm} \color{gray}{\textit{(by definition of expectation)}} \hspace{10mm}[6]
\end{eqnarray}
\]</span></p>
<p>To put this in words; the gradient of <span class="math inline">\(\log[u( x \:|\: \beta )]\)</span> is equivalent to the expected log-likelihood of the parameter <span class="math inline">\(\theta\)</span>, where the expectation is taken over the power posterior. Assuming we cannot derive this quantity analytically, we must turn to estimation methods. Let <span class="math inline">\(\theta_m^{\beta}\)</span> for <span class="math inline">\(m\in\{1,\dots,t\}\)</span> represent a series of independent draws from the power posterior with power <span class="math inline">\(\beta\)</span>. Then the expectation in (X) can be estimated using the quantity <span class="math inline">\(\widehat{D}_\beta\)</span>, defined as follows:</p>
<p><span class="math display">\[
\begin{eqnarray}
    \hspace{30mm} \widehat{D}_\beta = \frac{1}{t}\sum_{m=1}^t \log\left[ \mbox{Pr}( x \:|\: \theta_m^{\beta} ) \right] \;. \hspace{30mm}[7]
\end{eqnarray}
\]</span></p>
<p>Finally, this quantity must be related back to the model evidence. It is easy to demonstrate that the integral of (X) with respect to <span class="math inline">\(\beta\)</span> over the range <span class="math inline">\([0,1]\)</span> is equal to the logarithm of the model evidence:</p>
<p><span class="math display">\[
\begin{eqnarray}
    \int_0^1 \frac{d}{d\beta} \log[u( x \:|\: \beta )] \:d\beta &amp;=&amp; \log[u( x \:|\: \beta\!=\!1 )] - \log[u( x \:|\: \beta\!=\!0 )] \;, \\
    &amp;=&amp; \log[\mbox{Pr}(x)] \;,\hspace{60mm}[8]
\end{eqnarray}
\]</span></p>
<p>where we have made use of the fact that <span class="math inline">\(u( x \:|\: \beta\!=\!1 )\)</span> is equal to the model evidence, and <span class="math inline">\(u( x \:|\: \beta\!=\!0 )\)</span> is equal to the integral over the prior, which is 1 by definition. We cannot usually carry out this integral analytically, but we can approximate it using the values <span class="math inline">\(\widehat{D}_\beta\)</span> in a simple numerical integration technique, such as the <a href="https://en.wikipedia.org/wiki/Trapezoidal_rule">trapezoidal rule</a>. For example, if the values <span class="math inline">\(\beta_i=(i-1)/(r-1)\)</span> for <span class="math inline">\(i\!=\!\{1,\dots,r\}\)</span> represent a series of equally spaced powers spanning the interval <span class="math inline">\([0,1]\)</span>, where <span class="math inline">\(r\)</span> denotes the number of “rungs”" used (<span class="math inline">\(r\geq2\)</span>), then the integral in (X) can be approximated using</p>
<p><span class="math display">\[
\begin{eqnarray*}
    \hspace{30mm} \widehat{T} &amp;=&amp; \displaystyle\sum_{i=1}^{r-1} \frac{\tfrac{1}{2}(\widehat{D}_{\beta_{i+1}}+\widehat{D}_{\beta_i})}{r-1} \;, \\[4mm]
    &amp;=&amp; \tfrac{1}{r-1}\Big( \tfrac{1}{2}\widehat{D}_{\beta_1} + \tfrac{1}{2}\widehat{D}_{\beta_{r}} + \sum_{i=2}^{r-1} \widehat{D}_{\beta_i} \Big) \;.\hspace{30mm}[9]
\end{eqnarray*}
\]</span></p>
<p>There are two sources of error in this estimator. First, there is ordinary statistical error associated with replacing (X) by its Monte Carlo estimator, and second, there is discretisation error caused by replacing a continuous integral with a numerical approximation. Both of these sources of error are quantified in . In the example above the sampling variance of the estimator can be calculated using</p>
<p><span class="math display">\[
\begin{eqnarray}
    \hspace{30mm} \mbox{Var}[\widehat{T}] \approx \tfrac{1}{(r-1)^2}\Big( \tfrac{1}{4}\widehat{V}_{\beta_1} + \tfrac{1}{4}\widehat{V}_{\beta_{r}} + \sum_{i=2}^{r-1} \widehat{V}_{\beta_i} \Big) \;, \hspace{30mm}[10]
\end{eqnarray}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{eqnarray}
    \hspace{30mm} \widehat{V}_{\beta} = \tfrac{1}{t-1}\sum_{m=1}^t \left( \log\left[ \mbox{Pr}( x \:|\: \theta_m^{\beta} ) \right] - \widehat{D}_{\beta} \right)^2 \;. \hspace{30mm}[11]
\end{eqnarray}
\]</span></p>
<p>Notice that <span class="math inline">\(\widehat{V}_{\beta}\)</span> can be reduced by simply increasing the number of MCMC iterations (<span class="math inline">\(t\)</span>) used in the procedure. The discretisation error can also be kept within bounds. As noted by , the curve traced by <span class="math inline">\(\mbox{E}_{\scriptsize\theta \,|\, x, \beta}\Big[ \log[ \mbox{Pr}( x \:|\: \theta ) ] \Big]\)</span> is always-increasing with <span class="math inline">\(\beta\)</span>, meaning there are strict upper and lower bounds on the error that is possible given a series of known values distributed along this curve (and ignoring interactions between statistical error and discretisation error). These bounds can be quantified, leading to an estimate of the worst-case scenario discretisation error. However, in practice we find that these estimates are too pessimistic to be useful, and prefer to simply examine the shape of the estimated curve to ensure that sufficient rungs have been used to capture the curvature.</p>
</div>
<div id="generalised-thermodynamic-integration" class="section level2">
<h2 class="hasAnchor">
<a href="#generalised-thermodynamic-integration" class="anchor"></a>Generalised thermodynamic integration</h2>
<p>As described above, the two sources of error in TI are: 1) the statistical error in <span class="math inline">\(\widehat{D}_\beta\)</span>, and 2) the discretisation error caused by approximating a smooth curve with with a series of linear sections. The first issue brings uncertainty and the second brings bias, with bias being arguably more of a problem. Crucially, both of these problems are exacerbated when the curve is steep, as this is where variance is highest (it is demonstrated in [REF] that the variance is proportional to the gradient of the curve), and also where discretisation error is highest due to cutting corners in the smooth curve. In practical applications the steepest point in the curve is often at <span class="math inline">\(\beta = 0\)</span>, where we go from the pure prior (with no influence of the data) to something in between the prior and the posterior. This can be seen in <a href="https://bobverity.github.io/MALECOT/articles/comparing-models.html">an earlier tutorial</a>, where plots produced by the <code><a href="../reference/plot_loglike.html">plot_loglike()</a></code> function have wide quantiles towards <span class="math inline">\(\beta = 0\)</span> and much tighter quantiles as we move towards <span class="math inline">\(\beta = 1\)</span>. TI could therefore be improved by reducing the contribution that low <span class="math inline">\(\beta\)</span> values make towards the final estimator.</p>
<p>We start by noting that there are many possible modifications of <span class="math inline">\(\mbox{Pr}(x \: | \: \theta)\)</span> that return the prior when <span class="math inline">\(\beta = 0\)</span> and the likelihood when <span class="math inline">\(\beta = 1\)</span>, with <span class="math inline">\(\mbox{Pr}(x \: | \: \theta)^\beta\)</span> being just one option. For example, replacing <span class="math inline">\(\beta\)</span> with <span class="math inline">\(\beta^\alpha\)</span>, where <span class="math inline">\(\alpha \in (0, \infty)\)</span>, we arrive at the following alternative form of the power posterior:</p>
<p><span class="math display">\[
\begin{eqnarray}
    \hspace{30mm} P_{\beta,\alpha}(\theta \:|\: x) &amp;= \dfrac{\mbox{Pr}( x \:|\: \theta)^{\beta^\alpha} \: \mbox{Pr}(\theta)}{u( x \:|\: \beta, \alpha)} \;. \hspace{30mm}[12]
\end{eqnarray}
\]</span></p>
<p>where <span class="math inline">\(u(x \:|\: \beta, \alpha)\)</span> is the following normalising constant:</p>
<p><span class="math display">\[
\begin{eqnarray*}
    \hspace{30mm} u( x \:|\: \beta, \alpha) &amp;= \displaystyle\int_{\scriptsize\theta} \mbox{Pr}( x \:|\: \theta)^{\beta^\alpha} \: \mbox{Pr}(\theta) \:d\theta \;. \hspace{30mm}[13]
\end{eqnarray*}
\]</span> It is no more difficult to obtain draws from this distribution than it is to obtain draws from the original power posterior distribution. Continuing with the standard TI derivation we obtain:</p>
<p><span class="math display">\[
\begin{eqnarray}
    \frac{d}{d\beta} \log[u( x \:|\: \beta, \alpha )] &amp;=&amp; \frac{1}{u( x \:|\: \beta )} \: \frac{d}{d\beta} u( x \:|\: \beta, \alpha ) \;, \\[4mm]
    &amp;=&amp; \frac{1}{u( x \:|\: \beta, \alpha )} \: \frac{d}{d\beta} \int_{\scriptsize\theta} \: \mbox{Pr}( x \:|\: \theta )^{\beta^\alpha} \: \mbox{Pr}( \theta ) \:d\theta \;, \\[4mm]
    &amp;=&amp; \frac{1}{u( x \:|\: \beta, \alpha )} \: \int_{\scriptsize\theta} \: \frac{d}{d\beta} \mbox{Pr}( x \:|\: \theta )^{\beta^\alpha} \: \mbox{Pr}( \theta ) \:d\theta \;, \\[4mm]
    &amp;=&amp; \dfrac{1}{u( x \:|\: \beta, \alpha )} \: \displaystyle\int_{\scriptsize\theta}  \alpha\beta^{\alpha-1} \mbox{Pr}( x \:|\: \theta )^{\beta^\alpha} \log[ \mbox{Pr}( x \:|\: \theta ) ] \: \mbox{Pr}( \theta ) \:d\theta \;, \\[4mm]
    &amp;=&amp; \int_{\scriptsize\theta} \alpha\beta^{\alpha-1} \log[ \mbox{Pr}( x \:|\: \theta ) ] \frac{\mbox{Pr}( x \:|\: \theta )^{\beta^\alpha} \mbox{Pr}( \theta ) }{u( x \:|\: \beta, \alpha )} \:d\theta \;, \\[4mm]
    &amp;=&amp; \int_{\scriptsize\theta} \alpha\beta^{\alpha-1} \log[ \mbox{Pr}( x \:|\: \theta ) ] P_{\beta,\alpha}(\theta \:|\: x) \:d\theta \;, \\[4mm]
    &amp;=&amp; \mbox{E}_{\scriptsize\theta \,|\, x, \beta, \alpha}\Big[ \alpha\beta^{\alpha-1} \log[ \mbox{Pr}( x \:|\: \theta ) ] \Big] \;. \hspace{30mm}[14]
\end{eqnarray}
\]</span></p>
<p>Notice that the log-likelihood in the expectation is now weighted by <span class="math inline">\(\alpha\beta^{\alpha-1}\)</span>. It is still the case that the integral of <span class="math inline">\(\frac{d}{d\beta} \log[u( x \:|\: \beta, \alpha )]\)</span> over the interval <span class="math inline">\([0,1]\)</span> brings us to the log-evidence, and so the rest of the TI method remains essentially unchanged. We can define the statistic</p>
<p><span class="math display">\[
\begin{eqnarray}
    \hspace{30mm} \widehat{D}_{\beta,\alpha} = \frac{1}{t}\sum_{m=1}^t \alpha\beta^{\alpha-1} \log\left[ \mbox{Pr}( x \:|\: \theta_m^{\beta^\alpha} ) \right] \;. \hspace{30mm}[15]
\end{eqnarray}
\]</span> for a range of <span class="math inline">\(\beta_i\)</span>, and we can carry out numerical integration over these values as before. The fact that the expectation in (X) is weighted by <span class="math inline">\(\alpha\beta^{\alpha-1}\)</span> means that values towards the prior end of the spectrum are down-weighted relative to values near the posterior end of the spectrum, and in particular the new path is constrained to equal zero at the point <span class="math inline">\(\beta = 0\)</span> whenever <span class="math inline">\(\alpha &gt; 1\)</span>. This results in a much smoother path, and reduces both statistical error and discretisation error.</p>
<p>In summary, what we refer to as <em>generalised</em> thermodynamic integration here is any method by which the power <span class="math inline">\(\beta\)</span> is replaced by <span class="math inline">\(f(\beta)\)</span> to achieve a different weighting over the elements of the integral path. [REF]. In MALECOT we use the function <span class="math inline">\(f(\beta) = \beta^\alpha\)</span> as above, and the argument <code>GTI_pow</code> in the <code><a href="../reference/run_mcmc.html">run_mcmc()</a></code> function is exactly equal to <span class="math inline">\(\alpha\)</span>.</p>
<div id="references" class="section level3">
<h3 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h3>
<p>Hug, Sabine, et al. “An adaptive scheduling scheme for calculating Bayes factors with thermodynamic integration using Simpson’s rule.” Statistics and Computing 26.3 (2016): 663-677.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#problem-definition">Problem definition</a></li>
      <li><a href="#thermodynamic-integration">Thermodynamic integration</a></li>
      <li><a href="#generalised-thermodynamic-integration">Generalised thermodynamic integration</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Bob Verity.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
