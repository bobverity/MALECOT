---
title: "GTI mathematical details"
author: "Bob Verity"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{"GTI mathematical details"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignette gives complete mathematical details of the model comparison methods implemented in MALECOT. It covers:

1. model evidence and Bayes factors
2. thermodynamic integration (TI)
3. generalised thermodynamic integration (GTI)

Most of this derivation can be found in [Supplementary Material 1](http://www.genetics.org/content/203/4/1827.supplemental) of (ref), which also contains derivations of common model comparison statistics.


### Problem definition

We start by assuming a set of models $\mathcal{M}_k$ for $k \in 1:K$, each of which has its own set of parameters $\theta_k$. The probability of the observed data $x$ conditional on the model and the parameters of the model (a.k.a. the likelihood) can be written $\mbox{Pr}(x \: | \: \theta_k, \mathcal{M}_k)$. We are interested in comparing models based on the *Bayesian evidence*, defined as the probability of the model integrated over all free parameters (a.k.a. the marginal likelihood of the model). This can be written

$$
\begin{align}
	\hspace{30mm} \mbox{Pr}(x \:|\: \mathcal{M}_k) = \int_{\scriptsize\theta_k} \mbox{Pr}(x \:|\: \theta_k, \mathcal{M}_k) \mbox{Pr}(\theta_k \:|\: \mathcal{M}_k) \:d\theta_k \;. \hspace{30mm}[1]
\end{align}
$$
Once we have this quantity it is straightforward to compute bayes factors as the ratio of evidence between models:

$$
\mbox{BF}(\mathcal{M}_i:\mathcal{M}_j) = \frac{\mbox{Pr}(x \:|\: \mathcal{M}_i)}{\mbox{Pr}(x \:|\: \mathcal{M}_j)} \;,
$$
or to combine the evidence with a prior over models to arrive at the posterior probability of a model:

$$
\mbox{Pr}(\mathcal{M}_k \:|\: x) = \frac{\mbox{Pr}(x \:|\: \mathcal{M}_k)}{ \sum_{i=1}^K \mbox{Pr}(x \:|\: \mathcal{M}_i) } \;.
$$
The challenging part in this analysis plan is the integral in (X), which is often extremely high-dimensional making it computationally infeasible by standard methods. Fortunately we can use advanced MCMC methods to arrive at asymptotically unbiased estimates of $\log[\mbox{Pr}(x \:|\: \mathcal{M}_k)]$, from which we can derive all other quantities of interest.

In the following sections $\mathcal{M}_k$ and $\theta_k$ are often shortened to $\mathcal{M}$ and $\theta$ for succinctness, in which case it is assumed that the model $\mathcal{M}$ is one of several to be compared.


### Thermodynamic integration

Thermodynamic Integration (TI) provides direct estimates of the (log) model evidence that are unbiased and have finite and quantifiable variance. The method exploits the "power posterior" \citep{friel2008marginal}, defined as follows:

$$
\begin{eqnarray}
	P_\beta(\theta \:|\: x, \mathcal{M}) &= \dfrac{\mbox{Pr}( x \:|\: \theta, \mathcal{M})^\beta \: \mbox{Pr}( \theta \:|\: \mathcal{M})}{u( x \:|\: \beta, \mathcal{M})} \;.
\end{eqnarray}
$$

where $u(x \:|\: \beta, \mathcal{M})$ is a normalising constant that ensures the distribution integrates to unity:

$$
\begin{eqnarray*}
	u( x \:|\: \beta, \mathcal{M}) &= \displaystyle\int_{\scriptsize\theta} \mbox{Pr}( x \:|\: \theta, \mathcal{M})^\beta \: \mbox{Pr}( \theta \:|\: \mathcal{M}) \:d\theta \;.
\end{eqnarray*}
$$

In subsequent expressions in this section the conditioning on the model $\mathcal{M}$ will be supressed for succinctness.

The crucial step in the TI method is the following derivation:

$$
\begin{eqnarray}
	\frac{d}{d\beta} \log[u( x \:|\: \beta )] &=& \frac{1}{u( x \:|\: \beta )} \: \frac{d}{d\beta} u( x \:|\: \beta ) \;, \hspace{56mm} \color{gray}{\textit{(by chain rule)}} \\[4mm]
	&=& \frac{1}{u( x \:|\: \beta )} \: \frac{d}{d\beta} \int_{\scriptsize\theta} \: \mbox{Pr}( x \:|\: \theta )^\beta \: \mbox{Pr}( \theta ) \:d\theta \;, \hspace{30mm} \color{gray}{\textit{(substituting in (X))}} \\[4mm]
	&=& \frac{1}{u( x \:|\: \beta )} \: \int_{\scriptsize\theta} \: \frac{d}{d\beta} \mbox{Pr}( x \:|\: \theta )^\beta \: \mbox{Pr}( \theta ) \:d\theta \;, \hspace{30mm} \color{gray}{\textit{(by Leibniz integral rule)}} \\[4mm]
	&=& \dfrac{1}{u( x \:|\: \beta )} \: \displaystyle\int_{\scriptsize\theta}  \mbox{Pr}( x \:|\: \theta )^\beta \log[ \mbox{Pr}( x \:|\: \theta ) ] \: \mbox{Pr}( \theta ) \:d\theta \;, \hspace{14mm} \color{gray}{\textit{(evaluating derivative)}} \\[4mm]
	&=& \int_{\scriptsize\theta} \log[ \mbox{Pr}( x \:|\: \theta ) ] \frac{\mbox{Pr}( x \:|\: \theta )^\beta \mbox{Pr}( \theta ) }{u( x \:|\: \beta )} \:d\theta \;, \hspace{30mm} \color{gray}{\textit{(rearranging)}} \\[4mm]
	&=& \int_{\scriptsize\theta} \log[ \mbox{Pr}( x \:|\: \theta ) ] P_\beta(\theta \:|\: x) \:d\theta \;, \hspace{42mm} \color{gray}{\textit{(substituting in (X))}} \\[4mm]
	&=& \mbox{E}_{\scriptsize\theta \,|\, x, \beta}\Big[ \log[ \mbox{Pr}( x \:|\: \theta ) ] \Big] \;. \hspace{54mm} \color{gray}{\textit{(by definition of expectation)}}
\end{eqnarray}
$$

In other words, the gradient of $\log[u( x \:|\: \beta )]$ is equivalent to the expected log-likelihood of the parameter $\theta$, where the expectation is taken over the power posterior. Assuming we cannot derive this quantity analytically, we must turn to estimation methods. Let $\theta_m^{\beta}$ for $m\in\{1,\dots,t\}$ represent a series of independent draws from the power posterior with power $\beta$. Then the expectation can be estimated using the quantity $\widehat{D}_\beta$, defined as follows:

$$
\begin{eqnarray}
	\label{EQ_pathPoint}
	\widehat{D}_\beta = \frac{1}{t}\sum_{m=1}^t \log\left[ \mbox{Pr}( x \:|\: \theta_m^{\beta} ) \right] \;.
\end{eqnarray}
$$

Finally, this quantity must be related back to the model evidence. It is easy to demonstrate that the integral of \eqref{EQ_MainExpectation} with respect to $\beta$ over the range $[0,1]$ is equal to the logarithm of the model evidence:

$$
\begin{eqnarray}
	\notag \int_0^1 \frac{d}{d\beta} \log[u( x \:|\: \beta )] \:d\beta &=& \log[u( x \:|\: \beta\!=\!1 )] - \log[u( x \:|\: \beta\!=\!0 )] \;, \\
	\label{EQ_integral}
	&=& \log[\mbox{Pr}(x)] \;,
\end{eqnarray}
$$

where we have made use of the fact that $u( x \:|\: \beta\!=\!1 )$ is equal to the model evidence, and $u( x \:|\: \beta\!=\!0 )$ is equal to the integral over the prior, which is 1 by definition. We cannot usually carry out this integral analytically, but we can approximate it using the values $\widehat{D}_\beta$ in a simple numerical integration technique, such as the trapezium rule. For example, if the values $\beta_i=(i-1)/(r-1)$ for $i\!=\!\{1,\dots,r\}$ represent a series of equally spaced powers spanning the interval $[0,1]$, where $r$ denotes the number of `rungs' used ($r\geq2$), then the integral in \eqref{EQ_integral} can be approximated using 

$$
\begin{eqnarray*}
	\widehat{T} &=& \displaystyle\sum_{i=1}^{r-1} \frac{\tfrac{1}{2}(\widehat{D}_{\beta_{i+1}}+\widehat{D}_{\beta_i})}{r-1} \;, \\[4mm]
	&=& \tfrac{1}{r-1}\Big( \tfrac{1}{2}\widehat{D}_{\beta_1} + \tfrac{1}{2}\widehat{D}_{\beta_{r}} + \sum_{i=2}^{r-1} \widehat{D}_{\beta_i} \Big) \;.
\end{eqnarray*}
$$

There are two sources of error in this estimator. First, there is ordinary statistical error associated with replacing \eqref{EQ_MainExpectation} by its Monte Carlo estimator, and second, there is discretisation error caused by replacing a continuous integral with a numerical approximation. Both of these sources of error are quantified in \citet{lartillot2006computing}. In the example above the sampling variance of the estimator can be calculated using

$$
\begin{eqnarray}
	\mbox{Var}[\widehat{T}] \approx \tfrac{1}{(r-1)^2}\Big( \tfrac{1}{4}\widehat{V}_{\beta_1} + \tfrac{1}{4}\widehat{V}_{\beta_{r}} + \sum_{i=2}^{r-1} \widehat{V}_{\beta_i} \Big) \;,
\end{eqnarray}
$$

where

$$
\begin{eqnarray}
	\widehat{V}_{\beta} = \tfrac{1}{t-1}\sum_{m=1}^t \left( \log\left[ \mbox{Pr}( x \:|\: \theta_m^{\beta} ) \right] - \widehat{D}_{\beta} \right)^2 \;.
\end{eqnarray}
$$

Notice that $\widehat{V}_{\beta}$ can be reduced by simply increasing the number of MCMC iterations ($t$) used in the procedure. The discretisation error can also be kept within bounds. As noted by \citet{friel2013improving}, the curve traced by $\mbox{E}_{\scriptsize\theta \,|\, x, \beta}\Big[ \log[ \mbox{Pr}( x \:|\: \theta ) ] \Big]$ is always-increasing with $\beta$, meaning there are strict upper and lower bounds on the error that is possible given a series of known values distributed along this curve (and ignoring interactions between statistical error and discretisation error). These bounds can be quantified, leading to an estimate of the worst-case scenario discretisation error. In practice we find that these estimates are too pessimistic to be useful, and prefer to simply examine the shape of the estimated curve to ensure that sufficient rungs have been used to capture the curvature.

### Generalised thermodynamic integration
