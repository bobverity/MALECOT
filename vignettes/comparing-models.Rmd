---
title: "Tutorial 4: Comparing models (estimating K)"
author: "Bob Verity"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{"Comparing models (estimating K)"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
set.seed(3)
library(MALECOT)
reps <- 1e2
```

This vignette demonstrates model comparison in MALECOT. It covers:

1. Running thermodynamic MCMC
2. Checking convergence of thermodynamic MCMC
3. Estimating *K* via generalized thermodynamic integration (GTI)
4. Comparing parameter sets through the model evidence

This tutorial assumes some prior knowledge about MALECOT, so if you are completely new to the program we recommend working through the simpler [bi-allelic tutorial](https://bobverity.github.io/MALECOT/articles/tutorial-biallelic.html) first.

## Background on Bayesian model comparison

When carrying out Bayesian analysis it is useful to make a distinction between two types of analysis:

1. parameter estimation within a model
2. comparison between models

As an example, we might create a model $\mathcal{M}$ in which we write down the probability of the observed data $x$ as a function of the unknown allele frequencies $p$. In other words, we write down the likelihood $\mbox{Pr}(x \: | \: p, \mathcal{M})$.

In Bayesian parameter estimation we are trying to get at the posterior probability $\mbox{Pr}(p \: | \: x, \mathcal{M})$. We typically do this using MCMC, which produces a series of draws from the posterior distribution.

But what if we want to know the posterior probability of the *model*, rather than the parameters of the model? In other words, we want to know $\mbox{Pr}(\mathcal{M} \: | \: x)$. Calculating this quantity requires that we integrate over all unknown parameters:

$$\mbox{Pr}(\mathcal{M} \: | \: x) = \int \mbox{Pr}(\mathcal{M}, p \: | \: x) dp$$

This is often an extremely high-dimensional integral - for example in MALECOT there could easily be hundreds of unknown parameters - making it computationally infeasible by most methods. Regular MCMC cannot help us here either, because it only produces draws from the posterior distribution rather than normalised values.

One way around this is to use an advanced MCMC technique known as thermodynamic integration (TI). In TI we run multiple MCMC chains, each at a different "rung" on a temperature ladder. The hotter the chain, the flatter the target distribution. The log-likelihoods over all chains are then combined in a single calculation that - by what can only be described as mathematical magic! - is asymptotically equal to $\log[\mbox{Pr}(x \: | \: \mathcal{M})]$. We can then apply a prior over models, for example giving each model equal weight, to arrive at the desired posterior value $\mbox{Pr}(\mathcal{M} \: | \: x)$. *Generalised* thermodynamic integration (GTI) differs from regular TI in that it uses a slightly different calculation when combining information across rungs that leads to lower bias and higher precision. 

Hopefully this is enough background to run thermodynamic MCMC in MALECOT, and to understand the results. For those eager to understand all the mathematical details, see [this vignette](https://bobverity.github.io/MALECOT/articles/gti-mathematical-details.html).

## Running a thermodynamic MCMC

For the sake of this tutorial we will use simulated bi-allelic data drawn from $K = 3$ subpopulations. We create a new project and bind this data:

```{r}
# simulate data
mysim <- sim_data(data_format = "biallelic", n = 100, L = 24, K = 3)
```

```{r, echo=FALSE}
# secretly save/load data from file so results are consistent
#saveRDS(mysim, file = "../inst/extdata/tutorial4_mysim.rds")
mysim <- malecot_file("tutorial4_mysim.rds")
```

```{r}
# create project and bind data
myproj <- malecot_project()
myproj <- bind_data_biallelic(myproj, df = mysim$data, ID_col = 1, pop_col = 2)
```

For our first parameter set we will assume a very basic model with default parameters; which means a Poisson prior on COI, a flat prior on allele frequencies and no error estimation:

```{r}
# create parameter set
myproj <- new_set(myproj, name = "simple model")
```

We will then run the MCMC for values of $K$ from 1 to 5. What makes this thermodynamic MCMC rather than regular MCMC is the `rungs` argument, which dictates the number of rungs on the temperature ladder. We will opt for 10 rungs for now. Be warned - this MCMC will take considerably longer to run than regular MCMC, so be prepared to go make yourself a cup of tea!

```{r}
# run thermodynamic MCMC
myproj <- run_mcmc(myproj, K = 1:5, burnin = reps, converge_test = 1e2,
                   samples = reps, rungs = 10, pb_markdown =  TRUE)
```

Notice that convergence times are much longer than under regular MCMC. This is bacause the MCMC is only deemed to have converged when *every* chain has converged, meaning we are only as strong as the weakest link. When running thermodynamic MCMC it is therefore important to keep an eye on convergence, and to increase the number of burn-in iterations if needed.

## Checking thermodynamic MCMC behaviour

There are more moving parts in thermodynamic MCMC, meaning we have more things to check. First, we should perform the same diagnostic checks as for regular MCMC:

```{r}
plot_loglike_dignostic(myproj, K = 3)
```

This uses the cold rung by default (i.e. the final rung), but we can use the `rung` argument to plot diagnostics for any given rung.

Running `get_ESS()` now prints out the effective sample size of every rung:

```{r}
get_ESS(myproj, K = 3)
```

We interpret ESS for hot chains the same way as for the cold chain - as the number of independent samples that we have obtained from the target distribution once autocorrelation has been accounted for. Therefore, if we see small values of the ESS (tens to hundreds as a rule of thumb) then we should be concerned that that particular rung has has not explored the space well, and we should repeat the analysis with a larger number of samples.

A useful plotting function when working with multiple rungs is the `plot_loglike()` function, which plots the 95% quantiles of the log-likelihood at each rung:

```{r}
plot_loglike(myproj, K = 3)
```

This plot should be always-increasing from left to right, aside from small variations due to the random nature of MCMC. If any rungs stand out from the overall pattern then it is worth running `plot_loglike_diagnostic()` on these particular rungs. As with all other checks, this should be performed on every explored value of $K$. This plot can also be very useful when it comes to ensuring good mixing, as described in [another vignette](todo).

The GTI method takes the log-likelihood values above and multiplies them by custom weights, leading to a GTI "path". The area between this path and zero then makes up our estimate of the log-evidence. We can visualise this path using the `plot_GTI_path()` function:

```{r}
plot_GTI_path(myproj, K = 3)
```

As our evidence estimate is derived from the area between the line and zero, it is important that this path is not too jagged. We are essentially approximating a smooth curve with a series of straight line segments, so if there are corners cut off by this approximation then the area will be too large or too small leading to a biased estimate. There are two ways that we can mitigate this:

1. Increase the number of rungs
2. Change the `GTI_pow` argument

Increasing the number of rungs will obviously lead to a smoother path, and it will also help with MCMC mixing, but it comes at the cost of slowing the MCMC down. The `GTI_pow` argument changes the curvature of the path by modifying the log-likelihood weights, with large values leading to a more concave path. Ideally we want to choose `GTI_pow` such that the path is as straight as possible, as this will lead to smallest difference between the true curve and the approximation. The plot above is straight enough (using the default value `GTI_pow = 3`) so there is no need to re-run the MCMC.

Once we are happy with our log-likelihood estimates and our GTI path, we can use our evidence estimates to compare values of $K$. The raw estimates can be plotted using the `plot_logevidence_K()` function, which plots 95% credible intervals of the log-evidence for each value of $K$:

```{r}
plot_logevidence_K(myproj)
```

We can see immediately that the model favors $K = 3$ in this case, which agrees with our simulation parameters. If we had seen large credible intervals at this stage then we would have to re-run the model with a larger number of `samples`, but in this case the intervals are non-overlapping and the signal is clear.

We can also use the `plot_posterior_K()` function to plot the posterior probability of each value of $K$, which is obtained by taking these raw estimates out of log space and applying an equal prior over $K$.

```{r}
plot_posterior_K(myproj)
```

This plot is often easier to interpret as it is in units of probability. In this case we can see that there is a >99% posterior probability that the data were drawn from 3 subpopulations. Again, if there are large credible intervals at this stage then it is worth re-running the MCMC with a larger number of `samples`, but here there is no need.


## Comparing different parameter sets

```{r}
# create parameter set
myproj <- new_set(myproj, name = "error model", estimate_error = TRUE)

myproj
```

```{r}
# run MCMC
myproj <- run_mcmc(myproj, K = 1:5, burnin = reps, converge_test = 1e2,
                   samples = reps, rungs = 10, pb_markdown =  TRUE)
```

```{r}
plot_loglike_dignostic(myproj, K = 3)
```

```{r, fig.height=3, fig.width=4}
plot_e(myproj, K = 3)
```

```{r, fig.height=5, fig.width=8}
plot_loglike(myproj, K = 3)

plot_GTI_path(myproj, K = 3)
```


```{r}
plot_logevidence_K(myproj)
```

```{r}
plot_posterior_K(myproj)
```

```{r}
plot_logevidence_model(myproj)
```

```{r}
plot_posterior_model(myproj)
```

